# awesome-nlp-and-xai

## Transformers

### Encoder Only

### Decoder Only

### Encoder + Decoder

### Wait for arrangement

1. Switch Transformers: Scaling To Trillion Parameter Models With Simple and Efficient Sparsity ([pdf](https://arxiv.org/pdf/2101.03961.pdf), [code](https://github.com/lab-ml/nn/tree/master/labml_nn/transformers/switch), notes)

2. RoBERTa: A Robustly Optimized BERT Pretraining Approach ([pdf](https://arxiv.org/pdf/1907.11692.pdf), [code](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md), notes)

3. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ([pdf](https://arxiv.org/pdf/1810.04805.pdf), [code](https://github.com/google-research/bert), notes)

4. ALBERT: A Lite BERT For Self-supervised Learning of Language Presentations ([pdf](https://arxiv.org/pdf/1909.11942.pdf), [code](https://github.com/google-research/ALBERT), notes)
