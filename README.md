# awesome-nlp-and-xai

## Transformers

### Encoder Only

### Decoder Only

### Encoder + Decoder

### Multitask Learning

### Transfer Learning

### Meta Learning

### Analysis

1. Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks? ([pdf](https://arxiv.org/pdf/2012.15180.pdf), code, notes)

2. What does BERT Learn from Multiple-Choice Reading Comprehension Datasets? ([pdf](https://arxiv.org/pdf/1910.12391.pdf))

3. Unnatural Language Inference ([pdf](https://arxiv.org/pdf/2101.00010.pdf), code, notes)

### Datasets / Benchmarks

1. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding ([pdf](https://arxiv.org/pdf/1804.07461.pdf), [code](https://github.com/nyu-mll/GLUE-baselines), notes)

2. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding SystemsCODE ([pdf](https://arxiv.org/pdf/1905.00537.pdf), [code](https://github.com/nyu-mll/jiant), notes)

3. Adversarial NLI: A New Benchmark for Natural Language Understanding ([pdf](https://arxiv.org/pdf/1910.14599.pdf), [code](https://github.com/facebookresearch/anli), notes)

4. 

# Survey

### Wait for arrangement

1. Switch Transformers: Scaling To Trillion Parameter Models With Simple and Efficient Sparsity ([pdf](https://arxiv.org/pdf/2101.03961.pdf), [code](https://github.com/lab-ml/nn/tree/master/labml_nn/transformers/switch), notes)

2. RoBERTa: A Robustly Optimized BERT Pretraining Approach ([pdf](https://arxiv.org/pdf/1907.11692.pdf), [code](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md), notes)

3. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ([pdf](https://arxiv.org/pdf/1810.04805.pdf), [code](https://github.com/google-research/bert), notes)

4. ALBERT: A Lite BERT For Self-supervised Learning of Language Presentations ([pdf](https://arxiv.org/pdf/1909.11942.pdf), [code](https://github.com/google-research/ALBERT), notes)

## Natural Language Understanding

### Question Answering

1. 
