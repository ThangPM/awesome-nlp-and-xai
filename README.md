# awesome-nlp-and-xai

## Natural Language Understanding

### Transformers

#### Encoder Only

#### Decoder Only

#### Encoder + Decoder

#### Multitask Learning

#### Transfer Learning

#### Meta Learning

#### Analysis

[comment]: ([pdf](), [code](), notes)


1. Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks? ([pdf](https://arxiv.org/pdf/2012.15180.pdf), code, notes)

2. A Primer in BERTology: What we know about how BERT works ([pdf](https://arxiv.org/pdf/2002.12327.pdf), notes)

Unnatural Language Inference ([pdf](https://arxiv.org/pdf/2101.00010.pdf), code, notes)

### Datasets / Benchmarks

1. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding ([pdf](https://arxiv.org/pdf/1804.07461.pdf), [code](https://github.com/nyu-mll/GLUE-baselines), notes)

2. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding SystemsCODE ([pdf](https://arxiv.org/pdf/1905.00537.pdf), [code](https://github.com/nyu-mll/jiant), notes)

3. Adversarial NLI: A New Benchmark for Natural Language Understanding ([pdf](https://arxiv.org/pdf/1910.14599.pdf), [code](https://github.com/facebookresearch/anli), notes)

4. The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics [comment]: ([pdf](https://arxiv.org/pdf/2102.01672.pdf), [code](), notes)



# Survey

### Wait for arrangement

1. Switch Transformers: Scaling To Trillion Parameter Models With Simple and Efficient Sparsity ([pdf](https://arxiv.org/pdf/2101.03961.pdf), [code](https://github.com/lab-ml/nn/tree/master/labml_nn/transformers/switch), notes)

2. RoBERTa: A Robustly Optimized BERT Pretraining Approach ([pdf](https://arxiv.org/pdf/1907.11692.pdf), [code](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md), notes)

3. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ([pdf](https://arxiv.org/pdf/1810.04805.pdf), [code](https://github.com/google-research/bert), notes)

4. ALBERT: A Lite BERT For Self-supervised Learning of Language Presentations ([pdf](https://arxiv.org/pdf/1909.11942.pdf), [code](https://github.com/google-research/ALBERT), notes)

## Natural Language Understanding

### Question Answering

1. 

[comment]: ([pdf](), [code](), notes)

## Multimodality

1. Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet ([pdf](https://arxiv.org/pdf/2101.11986.pdf), [code](https://github.com/yitu-opensource/T2T-ViT), notes)








